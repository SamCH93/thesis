\chapter*{Introduction}
\phantomsection
\addcontentsline{toc}{chapter}{\texorpdfstring{\phantom{IV}Introduction}{Introduction}}

\hfill
\begin{tabular}{p{0.6\textwidth}}
  ``\emph{Confirmation comes from repetition}. Any attempt to avoid this statement leads at least to failure and more probably to destruction.'' \\
  \hfill J.W. \citet[p. 84]{Tukey1969}
\end{tabular}

% \begin{center}
% \begin{minipage}{12cm}
%   { { { ``We can make judgments of initial probabilities and infer final ones,
%         or we can equally make judgments of final ones and infer initial ones by
%         \emph{Bayes's theorem in reverse}.'' }}}
% \end{minipage}
% \end{center}
% \begin{flushright}
%    \citet[p. 29]{Good1983}
% \end{flushright}


\section{Replication studies}

\begin{wrapfigure}{r}{0.29\textwidth}
  \centering
  \includegraphics[height = 6.9cm]{images/xkcdCrop.png}
  \caption*{Replication studies as \mbox{illustrated} by Randall Munroe
    \mbox{(\url{https://xkcd.com/242})}.}
\end{wrapfigure}

% How can we know if a finding from a scientific study is really true? For
% example, how can we know if the protective effect of a vaccine found in a study
% is real? The answer to such questions is of high importance to scientists,
% decision makers, and the general population. However, because any study result
% comes with uncertainty we can never know for sure.
How can we know whether we can trust a finding from a scientific study? For
example, how can we know whether the protective effect of a vaccine found in a
study is a genuine effect or just a spurious finding? The answer to such
questions is of high importance to scientists, decision makers, and the general
population. However, because every study result is subject to uncertainty, we
can never know for sure.

One way to come a little closer to the truth is to repeat the original study
with new subjects. Such a \emph{replication study} may yield similar results
which would make us more confident about the original finding, or may yield
conflicting results which would lower our confidence. Replication studies are
thus an essential part of the scientific process as they provide a means for
substantiating genuine research findings and refuting research findings which
occurred merely by chance. For this reason, ``successful'' replication is often
a requirement, for instance, for acceptance of newly proposed scientific
theories (\eg{} a new physical model) or the implementation of policies based on
scientific knowledge (\eg{} market approval of a drug). The results from
replication studies may thus have real world consequences, such as deciding
whether we receive the recommendation to get vaccinated (a consequence that
anyone who experienced the COVID-19 pandemic is very aware of).


Despite their tremendous importance, the traditional academic system has made it
unattractive for researchers to conduct replication studies; the currencies of
science -- publications, citations, and grant money -- are typically easier to
acquire by conducting novel research. This is because until recently, most
scientific journals hardly published any replication studies as the emphasis was
on ``innovation'' rather than on ``repetition'' \citep{Makel2012, Martin2017,
  Coiera2021}. To build a successful career, researchers thus often had no other
choice than to concentrate their efforts on producing novel and eye-catching
research results \citep{Binswanger2014, Moher2018}.

The perceived value of replication studies has changed over the past decade.
Earlier criticisms of low research standards \citep{Altman1994, Ioannidis2005}
were backed up by empirical evidence. For instance, pharmaceutical companies
reported surprisingly low replication rates from pre-clinical research
\citep{Begley2012} followed by later studies estimating that %\$28 billion
billions are wasted each year on flawed and non-replicable research in medicine
and the life sciences \citep{Chalmers2014, Freedman2015, Glasziou2018}.
Similarly, reports of fraud \citep{Wicherts2011} and questionable research
practices \citep{Wagenmakers2011, Simmons2011, John2012} sparked intense
discussion about the need for better research standards in psychology and the
social sciences. These discussions eventually culminated in large-scale
replication projects conducted by huge researcher consortia in fields such as
psychology \citep{Opensc2015, Klein2014, Klein2018, Protzko2020}, economics
\citep{Camerer2016}, the social sciences \citep{Camerer2018}, experimental
philosophy \citep{Cova2018}, and cancer biology \citep{Errington2021}.

Most of these large-scale replication projects confirmed what many researchers
had feared; carefully conducted replication studies often show less impressive
results than their original counterparts, and the replicability of research
findings is surprisingly low (how precisely replicability is defined will soon
be discussed in more detail). This realization led many to declare science as
being in a ``replication crisis''. Debates arose about whether or not the crisis
really existed and who or what was to blame \citep{Gilbert2016, Amrhein2019}.
Even the popular press became interested \citep[\eg{}][]{Carey2015, Kovic2016,
  Achenbach2017, Devlin2018}, so that also in the eyes of the public the
credibility of science became seriously threatened.

\begin{figure}[!htb]
<< "google-scholar-graph", fig.height = 3.5 >>=
## ## Data gathered on October 18 2022
## searchTerm <- "Replication Study"
## searchDate <- as.Date(x = "2022-10-18", format = "%Y-%m-%d", tz = "GMT")
## searchDateFormat <- format(searchDate, format = "%B %d %Y")
## resultsAnytime <- 114000
## scholarDF <- data.frame(year = seq(2022, 1990, -1),
##                         results = c(4530, 11400, 13600, 12200, 11300, 10700,
##                                     9880, 9220, 8660, 8200, 7500, 6700, 6040,
##                                     5370, 4680, 3890, 3380, 3070, 2790, 2520,
##                                     2310, 2190, 2120, 2020, 1920, 1840, 1730,
##                                     1490, 1270, 1230, 1140, 1070, 970),
##                         study = c(347000, 459000, 1710000, 938000, 2360000,
##                                   675000, 3000000, 688000, 3150000, 512000,
##                                   3630000, 451000, 3650000, 396000, 3680000,
##                                   389000, 3890000, 375000, 3950000, 336000,
##                                   4160000, 364000, 4240000, 385000, 3470000,
##                                   377000, 3120000, 335000, 2620000, 376000,
##                                   2440000, 317000, 2220000))
## ## plot data
## ggplot(data = filter(scholarDF, year < 2022),
##        aes(x = year, y = results)) +
##     geom_segment(aes(xend = year, yend = 0), alpha = 0.2, size = 0.4) +
##     geom_point() +
##     ## scale_y_continuous(breaks = seq(0, 15000, 2500)) +
##     scale_x_continuous(breaks = seq(1990, 2020, 5)) +
##     ## scale_y_log10() +
##     labs(x = "Year", y = paste0('Number of results \nfor "', searchTerm, '"')) +
##     theme_bw() +
##     theme(panel.grid.minor = element_blank(),
##           panel.grid.major.x = element_blank()## ,
##           ## panel.grid.major.y = element_line(colour = "grey", linetype = "dashed")
##           )

WebofScienceDF <- data.frame(year = seq(2022, 1990, -1),
                             results = c(139, 144, 121, 137, 111, 80,
                                         89, 63, 68, 92, 63, 60, 39,
                                         39, 41, 30, 21, 20, 19, 25,
                                         15, 9, 13, 20, 19, 16, 14,
                                         8, 17, 13, 21, 5, 12),
                             study = c(235996, 272173, 245994, 230009, 207625,
                                       195012, 174800, 164635, 158745,
                                       184473, 138785, 128779, 116890, 111229,
                                       102231, 95143, 86691, 83132, 94432,
                                       79807, 70388, 53983, 54893, 53480,
                                       51744, 51898, 50070, 46826, 44591,
                                       43097, 42816, 43277, 43277)) %>%
    mutate(proportion = results/study,
           se = sqrt(proportion*(1 - proportion)/study))

## plot data
plotA <- ggplot(data = WebofScienceDF, aes(x = year, y = results)) +
    geom_segment(aes(xend = year, yend = 0), alpha = 0.2, linewidth = 0.5) +
    geom_point(size = 0.7) +
    scale_x_continuous(breaks = seq(1990, 2020, 5)) +
    lims(y = c(0, 160)) +
    labs(x = "Year", y = 'Number of results for "replication study"') +
    theme_bw() +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major.x = element_blank())
plotB <- ggplot(data = WebofScienceDF, aes(x = year, y = proportion)) +
    geom_segment(aes(y = proportion - 2*se, yend = proportion + 2*se, xend = year),
                 size = 0.5, alpha = 0.4) +
    geom_point(size = 0.7) +
    scale_x_continuous(breaks = seq(1990, 2020, 5), name = "Year") +
    scale_y_continuous(labels = scales::percent,
                       name = bquote("Proportion (" %+-% "2SE)")) +
    theme_bw() +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major.x = element_blank())

ggarrange(plotA, plotB, ncol = 2)
@
\caption{Yearly number of results for search term ``replication study'' on Web
  of Science (left), and number of results normalized by the number of results
  for the search term ``study'' (right). The search was conducted on 2 December
  2022.}
\label{fig:scholar}
\end{figure}

In the midst of this crisis, % , particularly from the hard-hit
% field of psychology,
various reforms were implemented to prevent an escalation of the situation % and
% save the reputation of science
\citep[for an overview see \eg{}][]{Munafo2017}. For instance, many journals
adopted the ``registered report'' format \citep{Chambers2021} in which a study
proposal is peer reviewed \emph{before} the study is conducted, and which, if
accepted, gives provisional publication acceptance regardless of the study
outcome. Similarly, digital infrastructure platforms, such as Zenodo
(\url{https://zenodo.org}) or the Open Science Framework (\url{https://osf.io}),
were created to facilitate preregistration, preprints, code and data sharing,
all of which have substantially increased over the last decade
\citep{Kidwell2016, Nosek2018, Rawlinson2019}. The practice of conducting
replication studies has also gained popularity. Several academic journals and
funders are now explicitly promoting replication research \citep{NWO2016,
  NSL2018, Nature2022}. Figure~\ref{fig:scholar} (left) illustrates this trend
by the yearly number of results for the search term ``replication study''
obtained from the search engine Web of Science. The numbers have been rapidly
growing, especially since the mid-2000s. This trend could possibly be explained
by the fact that more research results are published each year but also when the
numbers are normalized by the number of results for the search term ``study'',
the increasing trend is still visible.


\subsection{Statistical aspects of replication studies}
Despite the increased interest in replication studies, the research community
has not yet agreed on one important question: When is a replication study
considered successful? For this reason, replication researchers typically report
the results from different methods for assessing replication success. For
example, the \citet[p. 11]{Opensc2015} states ``[t]here is no single standard
for evaluating replication success. We evaluated [replicability] using
significance and P values, effect sizes, subjective assessments of replication
teams, and meta-analyses of effect sizes''. In the following, I will give an
overview about these and other methods which have been used in practice.


\begin{table}[!htb]
  \centering
  \caption{Study-level summary statistics for an original and a replication
    study. The cumulative distribution function of the standard normal
    distribution is denoted by $\Phi(\cdot)$, and the $1 - \alpha$ quantile of
    the standard normal distribution is denoted by
    $\Phi^{-1}(1 - \alpha) = \z{\alpha}$. Confidence intervals and
    \textit{p}-values are based on the assumption that the effect estimate
    $\that$ is normally distributed around the unknown effect size $\theta$ with
    (known) variance equal to the squared standard error $\sigma^{2}$.}
  \label{tab:meta}
  \begin{tabular}{lcc}
    \toprule
    & Original study & Replication study \\
    \midrule
    effect estimate & $\that_o$ & $\that_{r}$ \\
    standard error & $\sigma_{o}$ & $\sigma_{r}$ \\
    $(1 - \alpha)$ confidence interval &
    $[\that_{o} \pm \z{\alpha/2} \sigma_{o}]$ &
    $[\that_{r} \pm \z{\alpha/2} \sigma_{r}]$ \\
    % $[\that_{o} - \z{\alpha/2} \sigma_{o}, \that_{o} + \z{\alpha/2} \sigma_{o}]$ &
    % $[\that_{r} - \z{\alpha/2} \sigma_{r}, \that_{r} + \z{\alpha/2} \sigma_{r}]$ \\
    $z$-value & $z_{o} = \that_{o}/\sigma_{o}$ & $z_{r} = \that_{r}/\sigma_{r}$ \\
    $p$-value (two-sided) & $p_{o} = 2\{1 - \Phi(|z_{o}|)\}$ & $p_{r} = 2\{1 - \Phi(|z_{r}|)\}$ \\
    % One-sided $p$-value & $p_{o} = 1 - \Phi(|z_{o}|)$ & $p_{r} = 2\{1 - \Phi(|z_{r}|)\}$ \\
    \bottomrule
  \end{tabular}
\end{table}

Most methods for analyzing replication studies can be formulated in terms of
study-level summary statistics as shown in Table~\ref{tab:meta}. % Effect
% estimates, standard error, confidence intervals, and $p$-values
All of these are routinely reported in research articles, and if one of them is
missing they typically can be calculated from the others. Using summary
statistics is also often the only possible way for conducting the replication
success analysis as the raw data from the original study may not be available to
the researchers conducting the replication study. The most important statistic
is the effect estimate $\that$. It provides an estimate of the underlying effect
size $\theta$ which quantifies the true effect or association of an
intervention/exposure with an outcome variable. Typical effect sizes are mean
differences and correlations (for continuous outcomes), odds ratios (for binary
outcomes), or hazard ratios (for time to event outcomes). Depending on the type
of effect size, a transformation might be required so that the assumption of
approximately normally distributed effect estimates around the unknown effect
size $\theta$ (for large enough sample sizes) is justified. This could be, for
instance, the Fisher $z$-transformation for correlations or the
log-transformation for odds/hazard ratios \citep[chapter 11]{Cooper2019}. The
associated standard error $\sigma$ represents the statistical uncertainty of the
estimate.\footnote{Typically, the available standard error $\sigma$ is only an
  estimate of the true standard error. However, in most parts of this thesis it
  will be assumed that $\sigma$ corresponds to the true standard error. This is
  the same assumption as in ordinary fixed-effects/random-effects meta-analysis,
  and reasonable if the sample size of the studies is not too small.} Under the
assumption of (asymptotic) normality, confidence intervals for $\theta$ and
$p$-values for testing the null hypothesis $H_{0} \colon \theta = 0$ can be
computed as shown in Table~\ref{tab:meta}.


\begin{table}[!htb]
  \centering
  \caption{Statistical criteria for assessing replication success which have
    been used in practice \citep{Opensc2015, Camerer2016, Camerer2018, Cova2018,
      Errington2021}.}
  \label{tab:rsdef}
  \begin{tabular}{p{0.21\textwidth}p{0.7\textwidth}}
    \toprule
    Criterion type & Description \\
    \midrule
    Significance & The original and replication $p$-values are smaller than
                   a threshold $\alpha$ and their effect estimates show
                   the same direction
                   (\mbox{$p_{o} < \alpha$}, \mbox{$p_{r} < \alpha$}, and
                   \mbox{$\sign(\that_{o}) = \sign(\that_{r})$}). Usually $\alpha = 5\%$.
    \\
                   & \\
    Meta-analytic significance & The meta-analytic $p$-value is smaller than a threshold
                                 $\alpha$ (\mbox{$p_{m} = 2\{1 - \Phi(|\that_{m}|/\sigma_{m})\} < \alpha$})
                                 where \mbox{$\that_{m} =
                                 (\that_{o}/\sigma_{o}^{2} + \that_{r}/\sigma^{2}_{r})\sigma^{2}_{m}$}
                                 is the  pooled effect estimate with standard error
                                 \mbox{$\sigma_{m} = (1/\sigma^{2}_{o} + 1/\sigma^{2}_{r})^{-1/2}$}.
                                 Usually $\alpha = 5\%$. \\
                   & \\
    Relative effect size & The effect estimate of the replication study goes in the same
                           direction as the original one and is at least $d_{\text{min}}$ times as large (\mbox{$d = \that_{r}/\that_{o} \geq d_{\text{min}}$}).
                           Usually $d_{\text{min}} = 1$. \\
                   & \\
    Confidence interval & The replication effect estimate is contained in the
                          $(1 - \alpha)$ original confidence interval
                          ($\that_{r} \in [\that_{o} \pm z_{\alpha/2}\sigma_{o}]$).
                          Sometimes, also defined as the original effect estimate is
                          contained in the $(1 - \alpha)$ replication confidence interval
                          ($\that_{o} \in [\that_{r} \pm z_{\alpha/2}\sigma_{r}]$).
                          Usually $\alpha = 5\%$. \\
                   & \\
    Prediction interval ($Q$-test) & The replication effect estimate is contained in its
                                     $(1 - \alpha)$ prediction interval
                                     ($\that_{r} \in [\that_{o} \pm z_{\alpha/2}\surd(\sigma_{o}^{2}
                                     + \sigma_{r}^{2})]$). Usually $\alpha = 5\%$.
                                     This criterion is equivalent to $p_{Q} \geq \alpha$ where
                                     $p_{Q}$ is the $p$-value from the meta-analytic $Q$-test
                                     $p_{Q} = 2\{1 - \Phi(\surd Q)\}$ with $Q$-statistic
                                     $Q = \sum_{i \in \{o, r\}} (\that_{i} - \that_{m})^{2}/\sigma_{i}^{2} = (\that_{o}- \that_{r})^{2}/(\sigma^{2}_{o} + \sigma^{2}_{r})$.\\
    \bottomrule
  \end{tabular}
\end{table}

Table~\ref{tab:rsdef} lists commonly used criteria for replication success in
terms of the summary statistics from Table~\ref{tab:meta}. The most popular
criterion is listed first and defines replication success by simultaneous
statistical significance of original and replication study along with their
effect estimates showing the same direction. This approach is also called the
``two-trials rule'' \citep{Senn2008} in drug regulation or ``vote-counting'' in
meta-analysis \citep{Cooper2019}. The criterion can similarly be defined through
one-sided $p$-values, so that the original and replication effect estimate are
not required to show the same direction as this is taken into account by the
one-sided $p$-values. Some replication projects \citep{Opensc2015,
  Errington2021} also defined replication success via simultaneous
non-significance of original and replication study ($p_{o} > \alpha$ and
$p_{r} > \alpha$). However, with this definition ``replication success'' can
almost always be achieved by conducting original and replication study with very
few samples so that the $p$-values are large. The approach is also logically
questionable as it could be seen as an instance of the ``absence of evidence
fallacy'' \citep{Altman1995} meaning the failure to find evidence against the
null hypothesis is erroneously interpreted as evidence for the null hypothesis.
Meta-analytic extensions of the significance approach define replication success
by significance of a combined effect estimate.\footnote{The meta-analytic
  approach can also be given a Bayesian interpretation: A flat prior
  distribution for the underlying effect size $\theta$ is updated by the data
  from original and replication study. Replication success defined by a
  meta-analytic $p$-value being smaller than $\alpha$ is then equivalent to
  replication success via a Bayesian posterior probability
  $\Pr(\theta > 0 \given \that_{o}, \that_{r}) > 1 - \alpha/2$, respectively,
  $\Pr(\theta < 0 \given \that_{o}, \that_{r}) > 1 - \alpha/2$, depending on the
  direction of the combined estimate.} Typically, the assumption of a common
underlying effect size is seen as reasonable so that fixed-effect meta-analysis
is used for pooling. Random-effects meta-analysis has primarily been used if
more than one replication study of the same original study are conducted since
replicators are often interested in understanding between-replication
heterogeneity \citep[\eg{} in][]{Klein2018}.

The remaining criteria in Table~\ref{tab:rsdef} put more emphasis on
compatibility in effect size between the original and the replication study. For
example, the relative effect estimate $d = \that_{r}/\that_{o}$ quantifies how
much the magnitude of the replication effect estimate changed compared to the
original, and the smaller $d$ is the smaller the degree of replication success.
Some projects also report a confidence interval for $d$ \citep{Camerer2016,
  Camerer2018}, while others ignore its uncertainty and make a binary cut at
$d_{\text{min}} = 1$ to define replication success \citep{Errington2021}. The
criteria based on confidence and prediction intervals define effect size
compatibility on an absolute scale. However, the criterion based on the original
confidence interval ignores the uncertainty from the replication, whereas the
criterion based on the replication confidence interval ignores the uncertainty
from the original study. The prediction interval criterion takes into account
both sources of uncertainty \citep{Patil2016}. Yet, declaring ``replication
success'' based on the prediction interval may be logically questionable due to
its connection to the meta-analytic $Q$-test.
% as it may also be seen as an instance of the ``absence of evidence fallacy''
% \citep{Altman1995};
That is, if the $p$-value from the $Q$-test is larger than $\alpha$ this is
equivalent to the replication effect estimate $\that_{r}$ being contained in its
$(1- \alpha)$ prediction interval. The null hypothesis of this test is defined
that the underlying effect sizes of original and replication study are the same
($H_{0} \colon \theta_{o} = \theta_{r}$), so a rejection of this null hypothesis
corresponds to demonstrating replication failure and not replication success.
Interpreting a failure to reject the null hypothesis as evidence for it is again
an instance of the ``absence of evidence fallacy''
\citep{Hedges2019}. % To establish replication success a
% test based on the null hypothesis $H_{0} \colon \theta_{o} \neq \theta_{r}$
% would be required \citep{Hedges2019}.
As in the case of defining replication success by simultaneous non-significance
of original and replication study, the mismatch of the null hypothesis of the
prediction interval criterion results in the undesirable property that
``replication success'' can almost always be achieved if the sample size of the
studies is small enough as the prediction interval becomes wider with larger
standard errors.

% The rapidly increasing conduct of replication studies has
% outpaced the development of methodology for their analysis which is perhaps the
% reason why many of these criteria have some logical flaws. For instance, the
% confidence interval criterion does only take into account the statistical
% variability of one of the two studies. As a result,

\subsubsection{Example ``Reproducibility Project: Cancer Biology''}
I will now illustrate the assessment of replication success on data from the
``Reproducibility Project: Cancer Biology'' \citep{Errington2021}. This
large-scale project attempted to replicate 53 landmark studies from the field of
cancer biology \citep[and would perhaps therefore better be named ``Replication
Project'', see][]{Goodman2016}. In the end, only 23 of the 53 studies could be
repeated due to various difficulties, such as, missing information from the
original studies or problems in conducting the experiments.
\citet{Errington2021} report that these experiments led to data on 158
quantitative effects. However, from the data which they provide only 132
quantitative effects are provided with original and replication standardized
mean difference effect estimates along with standard errors, and only they will
be used in the subsequent analyses.

\begin{figure}[!htb]
<< "RPCB-graph", fig.height = 3.5 >>=

## function to format CI
ciFormat. <- function(lower, upper) {
    paste0("[", format(signif(lower, 2), nsmall = 2),
           ", ", format(signif(upper, 2), nsmall = 2), "]")
}
ciFormat <- Vectorize(FUN = ciFormat.)

## function to format p-value
pFormat. <- function(p, digits = 2, dollar = FALSE, phantom = FALSE) {
    if (is.na(p)) {
        out <- NA
    } else {
        if (p > 0.0001) {
            out <- as.character(signif(p, digits = digits))
            if (phantom) out <- paste0("\\phantom{<~}", out)
        } else {
            out <- "< 0.0001"
        }
        if (dollar) {
            out <- paste0("$", out, "$")
        }
    }
    return(out)
}
pFormat <- Vectorize(FUN = pFormat.)

## function to format (relative) effect estimate
dFormat. <- function(x, digits = 2, phantom = FALSE, dollar = FALSE) {
    if (is.na(x)) {
        out <- NA
    } else {
        out <- as.character(signif(x, digits = digits))
        if (phantom && (x >= 0)) {
           out <- paste0("\\phantom{-}", out)
        }
        if (dollar) out <- paste0("$", out, "$")
    }
    return(out)
}
dFormat <- Vectorize(FUN = dFormat.)

## function to color code numerical value in LaTeX table based on threshold
tableFormat. <- function(x, outFun = pFormat, threshold = 0.05, col1 = "nicegreen",
                         col2 = "red") {
    if (is.na(x)) {
        out <- NA
    } else {
        if (x < threshold) col <- col1
        else col <- col2
        out <- paste0("\\textcolor{", col, "}{",
                      outFun(x, dollar = TRUE, phantom = TRUE), "}")
    }
    return(out)
}
tableFormat <- Vectorize(FUN = tableFormat., vectorize.args = "x")


za <- qnorm(p = 0.975)
rpcb <- read.csv("data/rpcb.csv") |>
    mutate(zo = smdo/so,
           zr = smdr/sr,
           c = so^2/sr^2,
           d = smdr/smdo,
           po2 = 2*(1 - pnorm(q = abs(zo))),
           pr2 = 2*(1 - pnorm(q = abs(zr))),
           sm = 1/sqrt(1/so^2 + 1/sr^2),
           smdm = (smdo/so^2 + smdr/sr^2)*sm^2,
           pm2 = 2*(1 - pnorm(q = abs(smdm/sm))),
           Q = (smdo - smdr)^2/(so^2 + sr^2),
           pQ = pchisq(q = Q, df = 1, lower.tail = FALSE),
           dFormat = tableFormat(x = d, outFun = dFormat,
                                 threshold = 1, col1 = "red", col2 = "nicegreen"),
           cioL = smdo - za*so,
           cioU = smdo + za*so,
           cirL = smdr - za*sr,
           cirU = smdr + za*sr,
           cimL = smdm - za*sm,
           cimU = smdm + za*sm,
           id = paste0("(", paper, ", ", experiment, ", ", effect, ")"),
           po2Format = tableFormat(x = po2),
           pr2Format = tableFormat(x = pr2),
           pm2Format = tableFormat(x = pm2),
           pQFormat = tableFormat(x = pQ),
           smdoFormat = paste("$", dFormat(smdo), "~",
                              ciFormat(lower = cioL, upper = cioU), "$"),
           smdrFormat = paste("$", dFormat(smdr), "~",
                              ciFormat(lower = cirL, upper = cirU), "$"),
           smdmFormat = paste("$", dFormat(smdm), "~",
                              ciFormat(lower = cimL, upper = cimU), "$"),
           rsig = factor(pr2 <= 0.05, levels = c(FALSE, TRUE),
                         labels = c("italic(p[r]) > 0.05",
                                    "italic(p[r]) <= 0.05")))
## compute sceptical p-value (TODO fix problems with NA)
ind <- !is.na(rpcb$zo) & !is.na(rpcb$zr) & !is.na(rpcb$c)
rpcb$ps2 <- NA
rpcb$ps2[ind] <- pSceptical(zo = rpcb$zo[ind], zr = rpcb$zr[ind], c = rpcb$c[ind],
                            alternative = "two.sided", type = "nominal")
rpcb$ps2Format <- NA
rpcb$ps2Format <- tableFormat(x = rpcb$ps2)

zoom <- c(-2, 12)
alphazoom <- 0.6
ltyzoom <- 3
rpcbWithoutNA <- na.omit(rpcb)
plotA <- ggplot(data = rpcbWithoutNA, aes(x = smdo, y = smdr)) +
    annotate(geom = "segment", x = zoom[1], xend = zoom[2],
             y = zoom[1], yend = zoom[1], alpha = alphazoom,
             lty = ltyzoom) +
    annotate(geom = "segment", x = zoom[1], xend = zoom[2],
             y = zoom[2], yend = zoom[2], alpha = alphazoom,
             lty = ltyzoom) +
    annotate(geom = "segment", x = zoom[1], xend = zoom[1],
             y = zoom[1], yend = zoom[2], alpha = alphazoom,
             lty = ltyzoom) +
    annotate(geom = "segment", x = zoom[2], xend = zoom[2],
             y = zoom[1], yend = zoom[2], alpha = alphazoom,
             lty = ltyzoom) +
    geom_abline(intercept = 0, slope = 1, alpha = 0.3) +
    geom_hline(yintercept = 0, lty = 2, alpha = 0.9, size = 0.4) +
    geom_vline(xintercept = 0, lty = 2, alpha = 0.9, size = 0.4) +
    geom_point(aes(fill = rsig), shape = 21, alpha = 0.8) +
    geom_rug(aes(color = rsig), size = 0.3, show.legend = FALSE) +
    labs(x = "Original effect estimate (SMD)",
         y = "Replication effect estimate (SMD)",
         fill = "") +
    coord_fixed(xlim = c(-5, 30), ylim = c(-5, 30)) +
    scale_fill_discrete(labels = scales::parse_format()) +
    theme_bw() +
    theme(panel.grid = element_blank(),
          legend.position = c(0.5, 0.835))

plotB <- ggplot(data = rpcbWithoutNA, aes(x = smdo, y = smdr)) +
    geom_abline(intercept = 0, slope = 1, alpha = 0.3) +
    geom_hline(yintercept = 0, lty = 2, alpha = 0.9, size = 0.4) +
    geom_vline(xintercept = 0, lty = 2, alpha = 0.9, size = 0.4) +
    geom_point(aes(fill = rsig), shape = 21, alpha = 0.8,
               show.legend = FALSE) +
    geom_rug(aes(color = rsig), size = 0.3, show.legend = FALSE) +
    labs(x = "Original effect estimate (SMD)",
         y = "Replication effect estimate (SMD)",
         fill = "") +
    coord_fixed(xlim = zoom, ylim = zoom) +
    theme_bw() +
    theme(panel.grid = element_blank())
ggpubr::ggarrange(plotA, plotB, ncol = 2, align = "v")
@
\caption{Results for 132 effects from the Reproducibility Project: Cancer
  Biology \citep{Errington2021} for which effect estimates and standard errors
  are available on standardized mean difference (SMD) scale. The right plot
  shows a zoomed-in view of the dotted area in the left plot. The
  \textit{p}-values are recomputed using a normal approximation. Two study pairs
  with original effect estimate larger than 80 are not shown.}
\label{fig:rpcb}
\end{figure}

Figure~\ref{fig:rpcb} shows the original versus the replication effect estimate
(on standardized mean difference scale) with the color indicating whether the
replication study was statistically significant at the 5\% level (two-sided). As
in most other replication projects, the majority of the replications show
smaller effect estimates compared to their original counterparts (mean relative
effect estimate $\bar{d} = \Sexpr{round(mean(rpcbWithoutNA[,"d"]), 2)}$). Many
of the replications also fail to achieve statistical significance at the 5\%
level. Specifically, from the
\Sexpr{sum(rpcbWithoutNA$po2 < 0.05)} effects which were significant in the original study only \Sexpr{ sum(rpcbWithoutNA$po2
  < 0.05 & rpcbWithoutNA$pr2 < 0.05 & rpcbWithoutNA$d > 0)} were also
significant in the replication study (in the same direction).

\begin{figure}[!htb]
<< "RPCB-table1", results = "asis", fig.height = 5.5 >>=
## show number of successes
alpha <- 0.05
dmin <- 1
rpcbUpset<- rpcb |>
    filter(!is.na(po2), !is.na(pr2), !is.na(smdo), !is.na(smdr),
           !is.na(so), !is.na(sr)) |>
    mutate("Significance" = as.numeric(po2 < alpha & pr2 < alpha & sign(smdo) == sign(smdr)),
           "Meta analytic \nsignificance" = as.numeric(pm2 < alpha),
           "Relative \neffect size" = as.numeric(d >= dmin),
           "Prediction \ninterval" = as.numeric(pQ > alpha))
nsig <- sum(rpcbUpset[,"Significance"], na.rm = TRUE)
nma <- sum(rpcbUpset[,"Meta analytic \nsignificance"], na.rm = TRUE)
nres <- sum(rpcbUpset[,"Relative \neffect size"], na.rm = TRUE)
npi <- sum(rpcbUpset[,"Prediction \ninterval"], na.rm = TRUE)

upset(rpcbUpset, sets = c("Significance", "Meta analytic \nsignificance",
                          "Relative \neffect size", "Prediction \ninterval"),
      order.by = "freq", mainbar.y.label = "Successful replications (combined criteria)",
      sets.x.label = "Successful replications \n(single criterion)", #, empty.intersections = "on")
      queries = list(list(query = intersects,
                          params = c("Significance",
                                     "Meta analytic \nsignificance",
                                     "Relative \neffect size",
                                     "Prediction \ninterval"),
                          color = "#1b9e77", active = TRUE)))
@
\caption{UpSet plot for data on 132 effects from the Reproducibility Project:
  Cancer Biology \citep{Errington2021} for which effect estimates and standard
  errors are available on standardized mean difference scale. Shown are the
  number of replication successes according to the different criteria from
  Table~\ref{tab:rsdef} and their combinations. A threshold of
  $\alpha = \Sexpr{round(100*alpha, 2)}\%$ and a relative effect size threshold
  of $d_{\text{min}} = \Sexpr{dmin}$ are used for replication success.}
\label{fig:upset}
\end{figure}

Figure~\ref{fig:upset} shows how many of the replications are successful
according to the criteria from Table~\ref{tab:rsdef} (except the confidence
interval criteria since they do not take into account the uncertainty from both
studies). For the total 132 replications, most successes occur for the
meta-analytic significance (\Sexpr{nma}) and the prediction interval criteria
(\Sexpr{npi}), followed by significance (\Sexpr{nsig}), and relative effect size
(\Sexpr{nres}). However, looking at the combinations of the criteria, only in
two replications are all criteria satisfied simultaneously (green).

A detailed view for a subset of the data from the project is given in
Table~\ref{tab:RPCB}. The subset is chosen such that from every original study
one effect and one replication of that effect is included. For no single study
pair in the table are all commonly used criteria satisfied simultaneously. For
instance, the first two replications satisfy the significance criterion
($p_{o} < 0.05$ and $p_{r} < 0.05$), meta analytic significance ($p_{m} < 0.05$)
and the $Q$-test/prediction interval criterion ($p_{Q} > 0.05$), yet the
replication effect estimate is smaller than the original one ($d < 1$).
Similarly, there is no single study pair for which all criteria indicate
replication failure. For example, the last replication (\#20) is a clear failure
with respect to the relative effect estimate ($d < 1$) and the significance
criteria ($p_{o} > 0.05$ and $p_{r} > 0.05$), yet the $Q$-test does not indicate
evidence for inconsistency of the two effect estimates ($p_{Q} > 0.05$).

Taken together, this analysis demonstrates that conclusions based on commonly
used replication success criteria often differ. It is not always clear cut
whether or not a replication is successful. Reducing replicability to a single
criterion without mentioning these difficulties, as often done by the popular
press \citep[\eg{} ``more than half of the findings did not hold up when
retested'' in][]{Carey2015}, is a simplification of the matter.

\begin{landscape}
\begin{table}
  \centering
  \caption{Subset of results from the Reproducibility Project: Cancer Biology
    \citep{Errington2021}. (P, X, E) denotes effect number E from experiment
    number X, from original paper number P. Shown are original $\that_{o}$,
    replication $\that_{r}$, and pooled effect estimate $\that_{m}$ with 95\%
    confidence intervals, variance ratio $c = \sigma^{2}_{o}/\sigma^{2}_{r}$,
    relative effect estimate $d = \that_{r}/\that_{o}$, original
    \textit{p}-value $p_{o}$, replication \textit{p}-value $p_{r}$,
    meta-analytic \textit{p}-value $p_{m}$, $Q$-test \textit{p}-value $p_{Q}$,
    and sceptical \textit{p}-value $p_{\mathrm{S}}$. A replication success
    (green) threshold of $\alpha = 5\%$ is used for the \textit{p}-values and
    $d_{\text{min}} = 1$ for the relative effect estimate.}
  \label{tab:RPCB}
\resizebox{1\linewidth}{!}{%
<< "RPCB-table2", results = "asis" >>=
## select a subset with one effect per paper
set.seed(7)
rpcbTable <- rpcb |>
    na.omit() |>
    ## filter(smdo < 80, smdr < 80) |>
    group_by(paper) |>
    sample_n(size = 1) |>
    ungroup() |>
    arrange(pr2) |>
    select(id,
           smdoFormat, ## so,
           smdrFormat, smdmFormat,
           c, dFormat, ## sr,
           po2Format, pr2Format, pm2Format, pQFormat, ps2Format
           )

rpcbXtable <- xtable(rpcbTable)
colnames(rpcbXtable) <- c("(P, X, E)",
                          "$\\that_o$ [95\\% CI]",
                          "$\\that_r$ [95\\% CI]",
                          "$\\that_m$ [95\\% CI]",
                          "$c$", #"$\\sigma^2_o/\\sigma^2_r$",
                          "$d$", #"$\\that_r/\\that_o$",
                          "$p_o$",
                          "$p_r$",
                          "$p_m$",
                          "$p_Q$",
                          "$p_{\\text{S}}$"
                          )
colnames(rpcbXtable) <- paste0("\\multicolumn{1}{c}{", colnames(rpcbXtable), "}")
print(rpcbXtable, booktabs = TRUE, floating = FALSE, include.rownames = TRUE,
      sanitize.text.function = function(x){x},
      size = "small")
@
%
}
\end{table}

\end{landscape}

\subsection{Reverse-Bayes assessment of replication studies}
In response to the lack of a standard criterion for replication success, various
methods have been proposed \citep[among others]{Verhagen2014, Simonsohn2015,
  Anderson2016, Patil2016, Johnson2016, Etz2016, vanAert2017, Ly2018, Harms2019,
  Hedges2019, Mathur2020, Held2020, Pawel2020, Bonett2020}. The focus of this
thesis is to refine and extend the proposal from \citet{Held2020} which combines
\emph{reverse-Bayes inference} and \emph{Bayesian model criticism} in a method
for assessing replication success. In the following, I will summarize its main
ideas and technical underpinnings.

\subsubsection{Reverse-Bayes inference}

Bayesian inference is an approach to statistical inference where Bayes' theorem
is used to make probability statements about unknown parameters based on the
observed data. The central quantity for doing so is the distribution of the
parameters conditional on the observed data, called the \emph{posterior
  distribution}. It can be obtained from Bayes' theorem,
\begin{align*}
  f(\theta \given \text{data})
  = f(\theta) \times \frac{f(\text{data} \given \theta)}{f(\text{data})},
\end{align*}
meaning that the \emph{prior distribution} for the parameter $\theta$ with
probability density/mass function $f(\theta)$ is multiplied by the (normalized)
likelihood of the data, also know as \emph{Bayesian updating}. Parameter values
which increase the likelihood of the data become more likely \emph{a posteriori}
but they are also weighted by their \emph{a priori} plausibility through the
prior. As such, Bayesian inference provides a formal way for combining
information from the data at hand with external knowledge encoded in the prior.

Many consider the prior to be also the weak point of Bayesian inference since it
is unclear how it should be specified in the absence of external knowledge. The
\emph{reverse-Bayes} approach, first proposed by \citet{Good1950}, is one way of
dealing with this issue. The idea is to reverse Bayes' theorem as
\begin{align*}
  f(\theta) = f(\theta \given \text{data}) \times
  \frac{f(\text{data})}{f(\text{data} \given \theta)}
\end{align*}
and instead ``downdate'' a posterior with the observed data. So, in contrast to
the conventional ``forward-Bayes'' approach where we start with a prior, update
it with the data, and end up with a posterior, the reverse-Bayes approach starts
with the posterior and ends up with the prior. For example, the posterior may be
specified to indicate evidence for/against a particular hypothesis under
investigation. Reverse-Bayes inference then revolves around the question of
whether the resulting prior is plausible in light of external knowledge, and if
so, this could be seen as support for the specified posterior.

To illustrate reverse-Bayes inference, let us return to the replication setting.
Assume we want to conduct inference about the unknown effect size $\theta$ based
on the effect estimate from the original study $\that_{o}$ and its standard
error $\sigma_{o}$. We will assume that $\that_{o}$ is normally distributed
around the unknown effect size $\theta$ with (known) variance equal to its
squared standard error $\sigma^{2}_{o}$, here and henceforth denoted by
$\that_{o} \given \theta \sim \Nor(\theta, \sigma^{2}_{o})$. Furthermore, we
specify a zero-mean normal prior with variance $\tau^{2}$ for the effect size
$\theta$, representing the position of a sceptic who does not believe in the
presence of a genuine effect. The ``stubbornness'' of the sceptic is determined
by how small the variance $\tau^{2}$ is chosen (the smaller $\tau^{2}$, the more
scepticism). Combining the sceptical prior with the likelihood produces a
posterior which is again normal
$\theta \given \that_{o}, \sigma_{o} \sim \Nor(\mu_{\text{post}}, \sigma^{2}_{\text{post}})$
with mean and variance
\begin{align*}
  &\mu_{\text{post}} = \frac{\that_{o}}{1 + \sigma^{2}_{o}/\tau^{2}}&
  &\text{and}&
  &\sigma^{2}_{\text{post}} = \frac{1}{1/\sigma^{2}_{o} + 1/\tau^{2}}.&
\end{align*}
The associated $(1- \alpha)$ highest posterior density credible interval is
given by
\begin{align}
  \left[\mu_{\text{post}} \pm z_{\alpha/2} \, \sigma_{\text{post}}\right]
  \label{eq:postCI}
\end{align}
and if this credible interval excludes parameter values smaller/larger than zero
(depending on the orientation of the effect size) this may be interpreted as
evidence\footnote{For readers who do not agree with this notion of evidence, do
  not panic. In this thesis (Paper II and IV) we extend this reverse-Bayes
  procedure to use alternative measures of evidence, such as Bayes factors.} for
a genuine effect found in the original study.

Depending on how large the prior variance $\tau^{2}$ is chosen, the posterior
credible interval~\eqref{eq:postCI} will either include or exclude zero.
Different researchers may have different degrees of scepticism and may thus
choose different prior variances $\tau^{2}$. As a default choice,
\citet{Held2020} proposed to use the reverse-Bayes approach from
\citet{Matthews2001b}, that is, to determine the \emph{sufficiently sceptical
  prior variance} $\tau^{2}_{\alpha}$ so that the appropriate limit of the
$(1 - \alpha)$ credible interval is just fixed to zero. The resulting prior then
represents the beliefs of a sceptic who is just stubborn enough to not find the
original study convincing at level $\alpha$.

\begin{figure}[!htb]
<< "RBexample", fig.height = 3 >>=
## show reverse-Bayes for replications for a study and its three replications
exDat <- rpcb |>
    filter(id == "(9, 2, 5)")
to <- unique(exDat$smdo)
so <- unique(exDat$so)
zo <- to/so
tr <- exDat$smdr
sr <- exDat$sr

## do reverse-Bayes calculations
alpha <- 0.05
za <- qnorm(p = 1 - alpha/2)
sprior <- sqrt(so^2/(zo^2/za^2 - 1))
sposterior <- sqrt(1/(1/sprior^2 + 1/so^2))
mposterior <- sposterior^2*to/so^2
plotDF <- data.frame(t = c(to, tr, 0, mposterior),
                     s = c(so, sr, sprior, sposterior),
                     type = c("Original", rep("Replication", 3), "Prior",
                              "Posterior"))
plotDF$type <- factor(plotDF$type, levels = c("Original", "Posterior", "Prior",
                                              "Replication"))

## compute prior density
xprior <- seq(-3*sprior, 3*sprior, length.out = 200)
dprior <- dnorm(x = xprior, mean = 0, sd = sprior)
dpriorScaled <- 0.75*dprior#*max(dprior)

## compute posterior density
xpost <- seq(mposterior - 3*sposterior, mposterior + 3*sposterior,
             length.out = 200)
dpost <- dnorm(x = xpost, mean = mposterior, sd = sposterior)
dpostScaled <- 0.75*dpost#*max(dpost)

## prior data conflict assessment
pbox <- pchisq(q = tr^2/(sr^2 + sprior^2), df = 1, lower.tail = FALSE)
pboxFormat <- pFormat(p = pbox)
pboxLabel <- paste("italic(p)['Box'] == ", pboxFormat)
plotDF$Label <- c(NA, pboxLabel, NA, NA)
dodgewidth <- 0.5

ggplot(data = plotDF, aes(x = type, y = t)) +
    geom_hline(yintercept = 0, lty = 2, alpha = 0.3) +
    geom_pointrange(aes(ymin = t - za*s, ymax = t + za*s, col = type),
                    fatten = 3,
                    position = position_dodge2(width = dodgewidth),
                    show.legend = FALSE) +
    annotate("segment", x = 1.81, y = -0.5, xend = 1.97, yend = -0.05,
             alpha = 0.6,
             arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("text", x = 1.85, y = -0.6, size = 3, alpha = 0.6,
             label = paste0(round(100*alpha/2, 2), "% quantile fixed at zero")) +
    annotate("segment", x = 2.8, y = -0.5, xend = 2.96, yend = -0.05,
             alpha = 0.6,
             arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("text", x = 2.75, y = -0.6, size = 3, alpha = 0.6,
             label = "fixed at zero") +
    annotate("segment", x = 1.6, y = 2.25, xend = 2.7, yend = 2.25,
             alpha = 0.6,
             arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("text", x = 2.1, y = 2.45, size = 3, alpha = 0.6,
             label = "reverse-Bayes analysis") +
    annotate("segment", x = 3, y = 2.25, xend = 4, yend = 2.25,
             alpha = 0.6,
             arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
    annotate("text", x = 3.45, y = 2.45, size = 3, alpha = 0.6,
             label = "assess prior-data conflict") +
    annotate("path", x = 3 + dpriorScaled, y = xprior, color = 2, alpha = 0.5) +
    annotate("path", x = 2 + dpostScaled, y = xpost, color = 4, alpha = 0.5) +
    geom_text(aes(label = Label, y = t - za*s*0.9),
              position = position_dodge2(width = dodgewidth),
              parse = TRUE, size = 2.75, alpha = 0.8, hjust = -0.08) +
    labs(x = NULL, y = "Effect size") +
    scale_color_manual(values = c("Original" = 1, "Replication" = 1,
                                  "Posterior" = 4, "Prior" = 2)) +
    theme_bw() +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major.x = element_blank(),
          axis.text.x = element_text(size = 11))
@
\caption{Illustration of reverse-Bayes assessment of replication success using
  data from the original study (Paper 9, Experiment 2, Effect 5) and its three
  replication studies from the Reproducibility Project: Cancer Biology
  \citep{Errington2021}. Shown are effect estimates and prior/posterior means
  with \Sexpr{100*(1-alpha)}\% confidence/credible interval. The original finding is
  challenged with a sceptical prior, sufficiently concentrated around zero so
  that the resulting posterior is no longer convincing at level
  $\alpha = \Sexpr{100*(alpha)}\%$. Prior predictive \textit{p}-values $p_{\text{Box}}$ are
  computed for quantifying prior-data conflict.}
\label{fig:RBillustration}
\end{figure}

Figure~\ref{fig:RBillustration} illustrates the derivation of the sufficiently
sceptical prior variance for an original study included in the ``Reproducibility
Project: Cancer Biology'' with standardized mean difference effect estimate
$\that_{o} = \Sexpr{round(to, 2)}$ and standard error
$\sigma_{o} = \Sexpr{round(so, 2)}$. We see that the sufficiently sceptical
prior with variance $\tau_{\alpha}^{2} = \Sexpr{round(sprior, 2)}^{2}$ produces
a posterior with $1 - \alpha = \Sexpr{round(100*(1 - alpha), 2)}\%$ credible
interval fixed to zero, so that the original finding is rendered no longer
convincing at level $\alpha = \Sexpr{round(100*alpha, 2)}$\%.

\citet{Held2019a} showed that the sufficiently sceptical prior variance
$\tau^{2}_{\alpha}$ for a level $\alpha$ is available in closed-form
\begin{align}
  \tau^{2}_{\alpha} =
  \begin{cases}
    \dfrac{\sigma^{2}_{o}}{\left(z_{o}^{2}/z_{\alpha/2}^{2}\right) - 1}
    & \text{if} ~ z_{o}^{2} > z_{\alpha/2}^{2}, \\
    \text{undefined} & \text{else.}
  \end{cases}
  \label{eq:tau2alpha}
\end{align}
From~\eqref{eq:tau2alpha} we see that convincing original studies (those with
large absolute $z$-values $|z_{o}|$) require smaller sufficiently sceptical
prior variances to render the posterior no longer convincing for the same level
$\alpha$. Conversely, if the original study is not convincing enough (if
$|z_{o}| \leq z_{\alpha/2}$) the sufficiently sceptical prior variance is
undefined meaning that the data provide so little evidence that no scepticism is
required to make them unconvincing.

This shows that the reverse-Bayes approach based on sceptical priors can be used
to formally challenge the finding of an original study. However, once this
sceptical prior is determined, the question becomes whether it is plausible in
light of external data. A natural candidate for answering the question is data
from a replication study. In the following, I will show how Bayesian model
criticism can be used for doing so.


% \begin{figure}[!htb]
%   \centering
%   \begin{tikzpicture}[node distance = 5em, thick]
%     % nodes %
%     \node[text width=5em, text centered] (data) {Likelihood $f(\text{data} \,|\, \theta)$};
%     \node[text width=3em, text centered] (prior) [left=6.5em of data] {Prior $f(\theta)$};
%     \node[text width=5em, text centered] (post) [right=5em of data] {Posterior $f(\theta \,|\, \text{data})$};
%     \node[text width=10em, text centered] (RBinference) [below of=prior] {Reverse-Bayes inference};
%     \node[text width=10em, text centered] (FBinference) [below of=post] {Forward-Bayes inference};

%     % edges %
%     \draw [->, dotted] (prior) to [out=45, in=135, looseness=0.3] node[above]{Bayesian updating} (post);
%     \draw [->, dashed] (post) to [out=225, in=315, looseness=0.3] node[below]{Bayesian downdating} (prior);
%     \draw [->, dashed] (prior) to [out=270, in=90, looseness=0] (RBinference);
%     \draw [->, dotted] (post) to [out=270, in=90, looseness=0] (FBinference);
%   \end{tikzpicture}
%   \caption{Schematic illustration of forward-Bayes and reverse-Bayes inference
%     for an unknown parameter $\theta$.}
%   \label{fig:illustrationRB}
% \end{figure}

\subsubsection{Bayesian model criticism}
Model criticism describes the assessment of compatibility between observed data
and their assumed statistical model. If incompatibility is diagnosed, this
alarms the data analyst that inferences based on the model may be invalid and
modifications may be required. A formal framework for Bayesian model criticism
was first introduced by \citet{Box1980}. To understand whether a Bayesian model
M consisting of a joint distribution for parameter $\theta$ and data is
adequate, Box gave the following fundamental decomposition of the joint
distribution
\begin{align*}
  f(\theta, \text{data} \given \text{M})
  % &= f(\theta \given \text{M}) \times f(\text{data} \given \theta, \text{M})
  % \label{eq:decomp1} \\
  &= f(\theta \given \text{data}, \text{M}) \times f(\text{data} \given \text{M}).
    % \label{eq:decomp2}.
\end{align*}
% He noted from the first decomposition~\eqref{eq:decomp1} that the model M may
% only be regarded as adequate if current belief about
He reasoned that inferences based on the left factor, the posterior distribution
$f(\theta \given \text{data}, \text{M})$, should only be trusted if the right
factor, the prior predictive distribution
\begin{align*}
  f(\text{data} \given \text{M})
  = \int f(\text{data} \given \theta, \text{M}) \,
  f(\theta \given \text{M}) \, \text{d}\theta
\end{align*}
is compatible with the observed data. If the model M was indeed adequate, the
empirical distribution of the observed data should be close to its predictive
distribution under the model M. On the other hand, if the empirical distribution
differed from the predictive distribution, this would imply that model M is
inadequate due to misspecification of the likelihood
$f(\text{data} \given \theta, \text{M})$ and/or misspecification of the prior
$f(\theta \given \text{M})$.


Based on these observations, Box proposed two general approaches for conducting
Bayesian model criticism. First, the predictive density of the observed data (or
the value of a ``checking function'' applied to the observed data) can be
compared to its reference distribution via a \emph{prior predictive $p$-value}
\begin{align}
  p_{\text{Box}}
  = \Pr\left\{f(\text{data} \given \text{M}) <
  f(\text{data} = \text{observed data} \given \text{M}) \given \text{M}\right\}.
  \label{eq:pppval}
\end{align}
The $p$-value $p_{\text{Box}}$ is the probability of obtaining data with lower
predictive density (``more surprising'' data) than the observed data, and the
lower $p_{\text{Box}}$, the more incompatibility between the observed data and
the assumed model M. This approach was used by \citet{Held2020}.
% The literature on Bayesian model criticism following \citet{Box1980} also
% mostly focused on prior predictive $p$-values. However, what has been mostly
% ignored is that Box also mentioned an alternative approach.
However, Box also mentioned a second approach which has mostly been forgotten.
If a second ``benchmarking'' model $\text{M}_{2}$ alternative to the model under
investigation $\text{M}_{1}$ is available, Box proposed that the \emph{prior
  predictive ratio}
\begin{align*}
  \text{PPR}_{\text{Box}}
  = \frac{f(\text{data} = \text{observed data} \given \text{M}_{1})}{
  f(\text{data} = \text{observed data} \given \text{M}_{2})},
\end{align*}
the ratio of predictive densities from the observed data under both models,
could be used to judge the relative adequacy of model $\text{M}_{1}$. Again, the
lower the prior predictive ratio $\text{PPR}_{\text{Box}}$, the less compatible
the observed data with the model $\text{M}_{1}$. Bayesian model criticism
approaches based on prior predictive ratios will be used in latter parts of this
thesis.\footnote{Some readers may have noted that the prior predictive ratio is
  also the Bayes factor \citep{Jeffreys1961, Good1958} contrasting model
  $\text{M}_{1}$ to $\text{M}_{2}$. The prior predictive ratio model criticism
  approach is therefore particularly useful in combination with reverse-Bayes
  procedures based on Bayes factors, as will be demonstrated later (Paper II and
  IV).}

% In general,
% a distinction can be made between checking for \emph{likelihood-data conflict}
% and checking for \emph{prior-data conflict}. In the following, we will assume
% that the likelihood is adequate and focus on the



We now return to the replication setting. Having obtained a sceptical prior
$\theta \sim \Nor(0, \tau^{2}_{\alpha})$ with sufficiently sceptical prior
variance $\tau^{2}_{\alpha}$ from~\eqref{eq:tau2alpha}, the aim is to assess its
adequacy in light of the data from a replication study. If we are able to show
that the prior is inadequate, this would demonstrate that scepticism regarding
the original finding is unjustified and that the original study provided
evidence for a genuine effect. Under the assumption of a normal likelihood for
the replication effect estimate, \ie{}
$\that_{r} \given \theta \sim \Nor(\theta, \sigma^{2}_{r})$, the prior
predictive distribution is given by
$\that_{r} \given \theta \sim \Nor(0, \sigma^{2}_{r} + \tau^{2}_{\alpha})$. As
the prior predictive distribution is symmetric around zero, the prior predictive
$p$-value~\eqref{eq:pppval} is
% the probability of observing replication effects larger in absolute value than
% the observed one $\that_{r}$ under
\begin{align}
  p_{\text{Box}}
  = 2\left\{1 - \Phi\left(\frac{|\that_{r}|}{\sqrt{\sigma^{2}_{r} +
  \tau^{2}_{\alpha}}}\right)\right\}.
  \label{eq:pboxNorm}
\end{align}

<< "example2" >>=
## do reverse-Bayes calculations
alpha2 <- 0.1
za2 <- qnorm(p = 1 - alpha2/2)
sprior2 <- sqrt(so^2/(zo^2/za2^2 - 1))

## prior data conflict assessment
pbox2 <- pchisq(q = tr^2/(sr^2 + sprior2^2), df = 1, lower.tail = FALSE)
pboxFormat2 <- pFormat(p = pbox2)
@

Figure~\ref{fig:RBillustration} shows the prior predictive $p$-values
from~\eqref{eq:pboxNorm} computed for three example replication studies from the
Replication Project: Cancer Biology. We see that larger effect estimates show
smaller prior predictive $p$-values $p_{\text{Box}}$, indicating more prior-data
conflict. This is because the standard errors from all three replications are
roughly the same size, so that the distance between zero and the replication
effect estimate matters most. The $p$-values suggest that there is hardly any
conflict between the sceptical prior and the first two replications (those with
$p_{\text{Box}} = \Sexpr{pboxFormat[1]}$ and
$p_{\text{Box}} = \Sexpr{pboxFormat[2]}$), while the conflict is larger for the
third one (the one with $p_{\text{Box}} = \Sexpr{pboxFormat[3]}$).

% However, it is not clear how numerical values of $p_{\text{Box}}$ should be
% interpreted and below which value a replication should be declared
% ``successful''.
\citet{Held2020} defined replication success at level $\alpha$ by
\begin{align*}
  p_{\text{Box}} \leq \alpha.
\end{align*}
In words, replication success is established if there is more conflict between
the sceptical prior and the replication data than there was evidence against the
null hypothesis in the original study. For the examples in
Figure~\ref{fig:RBillustration}, all prior predictive $p$-values are larger than
the level $\alpha = 5\%$ used for computing the sufficiently sceptical prior
variance ($\tau^{2}_{\alpha} = \Sexpr{round(sprior, 2)}^{2}$), so neither of
them achieves replication success at level $\alpha = 5\%$. However, at a larger
level, \eg{} \mbox{$\alpha = \Sexpr{round(alpha2*100, 2)}\%$}, the corresponding
sufficiently sceptical prior variance would be smaller
($\tau^{2}_{\alpha} = \Sexpr{round(sprior2, 2)}^{2}$). Consequently, there would
be more conflict between the prior and the replication data,
% (in the same order as in Figure~\ref{fig:RBillustration}:
% $p_{\text{Box}} = \Sexpr{pboxFormat2[1]}$,
% $p_{\text{Box}} = \Sexpr{pboxFormat2[2]}$, and
% $p_{\text{Box}} = \Sexpr{pboxFormat2[3]}$),
so that the third replication would be successful at the less convincing level
\mbox{$\alpha = \Sexpr{round(alpha2*100, 2)}\%$} (since the prior predictive
$p$-value would be
$p_{\text{Box}} = \Sexpr{pboxFormat2[3]} < \Sexpr{round(alpha2*100, 2)}\%$).

<< "example3" >>=
## do reverse-Bayes calculations
alpha2 <- 0.1
za2 <- qnorm(p = 1 - alpha2/2)
sprior2 <- sqrt(so^2/(zo^2/za2^2 - 1))

## prior data conflict assessment
pbox2 <- pchisq(q = tr^2/(sr^2 + sprior2^2), df = 1, lower.tail = FALSE)
pboxFormat2 <- pFormat(p = pbox2)

zr <- tr/sr
c <- so^2/sr^2
ps <- pSceptical(zo = zo, zr = zr, c = c, alternative = "two.sided", type = "nominal")
psFormat <- pFormat(p = ps)
@
\subsubsection{The sceptical \textit{p}-value}
To remove the dependence on the level $\alpha$, \citet{Held2020} proposed to
determine the smallest level at which replication success can be established.
This level is called the \emph{sceptical $p$-value} $p_{\text{S}}$, and it is
available in closed-form
\begin{align*}
  p_{\text{S}}
  % = \inf_{\alpha \in (0, 1)} \left\{\alpha : p_{\text{Box}} \leq \alpha \right\}.
  = 2\left\{1 - \Phi(|z_{\text{S}}|)\right\}
\end{align*}
where
\begin{align*}
  z_{\text{S}}^{2}
  = \begin{cases}
    z^{2}_{\text{H}}/2 & \text{for} ~ c = 1 \\
    \left\{\left[z_{\text{A}}^{2}\left\{z_{\text{A}}^{2} +
          z_{\text{H}}^{2}(c - 1)\right\}\right]^{1/2}
      - z_{\text{A}}^{2}\right\}/(c - 1)
    & \text{for} ~ c \neq 1
  \end{cases}
\end{align*}
with arithmetic mean $z_{\text{A}}^{2} = (z^{2}_{o} + z^{2}_{r})/2$ and harmonic
mean $z_{\text{H}}^{2} = 2/(1/z^{2}_{o} + 1/z^{2}_{r})$ of the squared
$z$-statistics, and variance ratio $c = \sigma^{2}_{o}/\sigma^{2}_{r}$.
Replication success at level $\alpha$ is then equivalent to
$p_{\text{S}} \leq \alpha$. For instance, the sceptical $p$-values of the three
replication studies in Figure~\ref{fig:RBillustration} are
$p_{\text{S}} = \Sexpr{psFormat[1]}$, $p_{\text{S}} = \Sexpr{psFormat[2]}$, and
$p_{\text{S}} = \Sexpr{psFormat[3]}$ (from left to right), so we can see that
the third replication is unsuccessful at level $\alpha = 5\%$ but successful at
level $\alpha = 10\%$. However, the sceptical $p$-value does not necessarily
have to be dichotomized in this manner, but can also be interpreted as a
quantitative measure of replication success; the smaller $p_{\text{S}}$, the
higher the degree of replication success.

The sceptical $p$-value has several interesting properties \citep[Section
3]{Held2020}: First, it is always larger than the maximum of the original and
replication $p$-values ($p_{\text{S}} > \max\{p_{o}, p_{r}\}$), meaning that
both $p$-values have to be smaller than $\alpha$ such that replication success
at level $\alpha$ is possible. The sceptical $p$-value hence requires
\emph{both} studies to be sufficiently convincing on their own (in terms of
their $p$-values), similar to the significance criterion for replication
success. Second, if the $p$-values $p_{o}$ and $p_{r}$ remain fixed but the
relative effect estimate $d = \that_{r}/\that_{o}$ decreases, the sceptical
$p$-value increases, meaning that shrinkage of the replication effect estimate
is penalized ($p_{\text{S}} \uparrow 1$ as $d \downarrow 0$ for fixed $p_{o}$
and $p_{r}$). The sceptical $p$-value thus takes effect shrinkage into account,
unlike the significance criterion which can be achieved with any non-zero
replication effect estimate $\that_{r}$ provided the standard error $\sigma_{r}$
is small enough. This property is desirable in the replication setting as a
smaller replication effect estimate $\that_{r}$ may not be practically relevant
anymore, despite its statistical significance.

The results from the ``Reproducibility Project: Cancer Biology'' in
Table~\ref{tab:RPCB} illustrate these two properties. The third study fails to
achieve replication success at level $\alpha = 5\%$ with the sceptical
$p$-value, even though the replication study was highly convincing (the effect
estimate was almost five times as large as in the original study). Yet, as the
approach requires both studies to be convincing on their own -- and the original
study was not significant at the 5\% level -- replication success at this level
is impossible with the sceptical $p$-value. The second property is illustrated
by the fifth study. Here the original study was convincing and the replication
study also achieved significance. However, the effect estimate from the
replication was roughly 60\% smaller than the one from the original study, and
significance is merely achieved because the standard error was much smaller
(\ie{} around $1/\surd c \approx 1/2.8$ times smaller). The sceptical $p$-value
is therefore only $p_{\text{S}} = 0.25$, indicating hardly any evidence for
replication success.

This concludes the introduction to replication studies and reverse-Bayes methods
for their analysis. Some additional properties (\eg{} the null distribution of
the sceptical $p$-value) and extensions (\eg{} a one-sided version of the
sceptical $p$-value, power and sample size calculations) can be found in the
original article by \citet{Held2020}. The sceptical $p$-value and related
methods are implemented in the R package \texttt{ReplicationSuccess} available
on CRAN
(\url{https://cran.r-project.org/package=ReplicationSuccess}). % However, the
% relevant parts of the procedure have been presented.
In the following, I will discuss open questions and how they are addressed in
this thesis.


\section{Thesis contributions}
\label{sec:summary}
\raggedbottom

This thesis consists of six papers divided into three parts. The first part
(Paper I, II, and III) focuses on methodology for design and analysis of
replication studies. The second part (Paper IV and V) revolves around
reverse-Bayes methodology with a broader scope of applications than replication
studies. % Paper IV is a review paper about
% reverse-Bayes methodology and Paper V is a short comment on another article
% which proposed a reverse-Bayes method.
The last part (Paper VI) is about integrity issues in methodological research
with simulations studies. % The article lists and illustrates questionable
% research practices in simulation studies, and gives recommendations how to avoid
% them.

\subsection{Design and analysis of replication studies}
% The research from this parts extends

\startPaperSummary{PaperI}{%
  The assessment of replication success based on relative effect size}{%
  Leonhard Held, Charlotte Micheloud, Samuel Pawel \\
  \textit{The Annals of Applied Statistics}, 2022, 16(2), 706--720.
  doi:\href{https://doi.org/10.1214/21-AOAS1502}{10.1214/21-AOAS1502}.}

It is not clear how to interpret the sceptical $p$-value as it is not an
ordinary $p$-value (which has a uniform distribution under the corresponding
null hypothesis). Moreover, when the same level $\alpha$ as for the significance
criterion is used for dichotomizing the sceptical $p$-value, this leads to a
much more stringent criterion for replication studies than the significance
criterion, and many studies which intuitively seem successful will not satisfy
it.

In this article, we therefore look closer at the ``success region'' of the
sceptical $p$-value in terms of the relative effect estimate
$d = \that_{r}/\that_{o}$. This perspective leads to the proposal of a new
default level for thresholding the sceptical $p$-value called the \emph{golden
  level} $\alpha_{G}$ (named ``golden'' because the golden ratio appears in its
derivation). The golden level is defined through the property that for an
original study which was borderline significant ($p_{o} = \alpha$), replication
success based on $p_{\text{S}} \leq \alpha_{G}$ is only possible if the
replication effect estimate is at least as large as the original one
($d \geq 1$). For instance, for the significance level $\alpha = 5\%$, the
corresponding golden level is $\alpha_{G} = 12.3\%$. The behavior of the golden
level seems to align with common sense: for original studies which were already
convincing (in terms of their $p$-value) the effect estimate in the replication
study is allowed to shrink, to some extent, whereas for less convincing original
studies (those with $p$-values around the significance level) shrinkage is more
strongly penalized. We find that for replication studies with equal or larger
sample size than the original study, replication success based on the golden
level also has similar or improved frequentist properties (type-I error rate and
project power) compared to the standard significance criterion. Case studies
from four large-scale replication projects \citep{Opensc2015, Camerer2016,
  Camerer2018, Cova2018} illustrate the properties of the method. This extension
is now implemented as the default option in the R package
\texttt{ReplicationSuccess}.

L. Held had the idea to apply the sceptical $p$-value method to the data from
the four replication projects, which I collected for my master thesis. C.
Micheloud and L. Held came up with the golden level. L. Held wrote an initial
draft of the manuscript. C. Micheloud, L. Held, and I then iteratively worked on
the manuscript.

\startPaperSummary{PaperII}{%
  The sceptical Bayes factor for the assessment of replication success}{%
  Samuel Pawel, Leonhard Held \\
  \textit{Journal of the Royal Statistical Society: Series B (Statistical
    Methodology)}, 2022, 84(3), 879--911.
  doi:\href{https://doi.org/10.1111/rssb.12491}{10.1111/rssb.12491}.}

The reverse-Bayes approach from \citet{Held2020} is based on challenging the
original study with a sceptical prior so there is no longer evidence for an
effect. Evidence is quantified in terms of credible intervals and tail
probabilities. However, there exist also other measures of evidence, and it has
been a matter of long debates which is the most appropriate \citep[see
\eg{}][]{Berger1987, Casella1987, Royall1997, Berger2003, Benjamin2017,
  Lakens2018, Amrhein2019b}. In this paper, we extend the reverse-Bayes
assessment of replication studies to use Bayes factors \citep{Good1958,
  Jeffreys1961} for quantifying evidence and prior-data
conflict. % Bayes factors are also
% used for prior-data conflict assessment as suggested by \citep{Box1980}.
Similar to the sceptical $p$-value, the procedure leads to a single measure for
quantifying the degree of replication success, \emph{the sceptical Bayes
  factor}. Systematic comparisons show that the sceptical Bayes factor has
similar properties as the sceptical $p$-value due to the reverse-Bayes approach
underlying both methods (\eg{} it penalizes shrinkage of the replication effect
estimate and requires compelling evidence from both studies in terms of
$p$-values and Bayes factors, respectively). However, it is also shown that the
sceptical $p$-value suffers from a certain type of ``shrinkage paradox'', which
is avoided by the sceptical Bayes factor; when the $p$-value from the original
study goes to zero, replication success based on the sceptical $p$-value can be
achieved with any arbitrarily small replication effect estimate, whereas
replication success based on the sceptical Bayes factor poses a finite limit on
how much shrinkage is allowed. Technically, the procedure is more involved and
closed-form solutions for the sceptical Bayes factor are only available in
special situations. % and in terms of the
% ``Lambert W function'' \citep{Corless1996}.
The method is illustrated on data from the ``Social Sciences Replication
Project'' \citep{Camerer2018}, and implemented in the R package
\texttt{BayesRep} (\url{https://gitlab.uzh.ch/samuel.pawel/BayesRep}).

The idea to use Bayes factors instead of tail probabilities was suggested by
\citet{Consonni2019} and \citet{Pericchi2020} independently in response to the
original article by \citet{Held2020}. L. Held then implemented a first version
of the procedure for the grant application of this research project
\citep{Heldproposal2019}. I then worked out the technical and implementation
details, including closed-form solution for the sceptical Bayes factors,
asymptotic properties, type-I and type-II error rates, and non-normal
extensions. I wrote the initial draft of the manuscript and the R package.
Throughout, L. Held gave high-level feedback. I presented initial results at the
annual meeting of the GMDS and CEN-IBS (German Association for Medical
Informatics, Biometry and Epidemiology and the Central European Network of the
International Biometric Society) in 2020. L. Held presented the final results at
the ISBA (International Society for Bayesian Analysis) world meeting 2021.

\startPaperSummary{PaperIII}{%
  Bayesian approaches to designing replication studies}{%
  Samuel Pawel, Guido Consonni, Leonhard Held \\
  2022. arXiv preprint.
  doi:\href{https://doi.org/10.48550/arXiv.2211.02552}{10.48550/arXiv.2211.02552}.}

An important aspect in the design of replication studies is determining their
sample size. How exactly the sample size should be determined depends on which
method is used for the analysis of the replication data. Various approaches have
been proposed for doing so which are specifically tailored to certain analysis
methods. In this article, we provide a general Bayesian framework which applies
to any analysis method (including the sceptical $p$-value and the sceptical
Bayes factor). We show how the data from the original study and external
knowledge can be combined in a \emph{design prior} for the underlying model
parameters. Based on this design prior, predictions about the replication data
can then be computed, and the replication sample size can be chosen such that
the probability of replication success becomes as high as desired. We illustrate
Bayesian design of replication studies in the normal-normal hierarchical model
which provides sufficient flexibility for specification of design priors. Data
from a cross-laboratory replication project \citep{Protzko2020} are used for
illustrating our methods, which are available in the R package
\texttt{BayesRepDesign} (\url{https://github.com/SamCH93/BayesRepDesign}).

L. Held specified in the grant application of this research project
\citep{Heldproposal2019} that we will investigate power and sample size
calculations for the sceptical $p$-value and the sceptical Bayes factor. In
Paper II, I already derived the power function of the sceptical Bayes factor in
closed-form for two types of design priors. After its completion, I generalized
the result to any design prior in the normal-normal hierarchical model, and
started working on this manuscript. I presented a first draft to L. Held and G.
Consonni in the beginning of 2021. \mbox{G. Consonni} then helped developing the
methodology for multisite replication study design. I continued working on the
manuscript in 2022 and also wrote the accompanying R package. Throughout, L.
Held and G. Consonni gave high-level feedback.

\subsection{Reverse-Bayes methodology}

\startPaperSummary{PaperIV}{%
  Reverse-Bayes methods for evidence assessment and research synthesis}{%
  Leonhard Held, Robert Matthews, Manuela Ott, Samuel Pawel \\
  \textit{Research Synthesis Methods}, 2022, 13(3), 295--314.
   doi:\href{https://doi.org/10.1002/jrsm.1538}{10.1002/jrsm.1538}.}

While the popularity of Bayesian methods has been rapidly increasing since the
advent of modern computational methods in the 1990s, reverse-Bayes methods % , such
% as the reverse-Bayes assessment of replication studies,
have remained largely unknown to statisticians and users of statistics alike. In
this article, we review reverse-Bayes history and methods to increase awareness
about the approach. Specifically, we summarize the work on reverse-Bayes by I.
J. Good \citep{Good1950}, who first proposed the idea. We then review methods
such as the \emph{Analysis of Credibility} from \citet{Matthews2001b,
  Matthews2018}, its extension to Bayes factors, and the \emph{False Positive
  Risk} from \citet{Colquhoun2017}. To illustrate these method, we use data from
a meta-analysis on the effect of corticosteroids on COVID-19 mortality.

L. Held and M. Ott started working on this article several years ago. When I
discovered the connection between the Analysis of Credibility and the fail-safe
$N$ method \citep{Rosenthal1979,Rosenberg2005}, L. Held suggested that it would
fit nicely into this manuscript, and that I should start to overhaul it. I
rewrote and expanded his initial draft, adding also a new section on
reverse-Bayes approaches with Bayes factors, largely based on the work from
Paper II. We then managed to recruit R. Matthews to also contribute. From that
point on the three of us iteratively worked on the manuscript and M. Ott gave
high-level feedback.

\startPaperSummary{PaperV}{%
  Comment on ``Bayesian additional evidence for decision making under small
sample uncertainty''}{%
   Samuel Pawel, Leonhard Held, Robert Matthews \\
  \textit{BMC Medical Research Methodology}, 2022, 22(149).
  doi:\href{https://doi.org/10.1186/s12874-022-01635-4}{10.1186/s12874-022-01635-4}.}

Shortly after the acceptance of Paper III, the article by \citet{Sondhi2021}
appeared. It proposed a novel reverse-Bayes method called \emph{Bayesian
  Additional Evidence}, and we noted some flaws in the article. This prompted us
to write a commentary. We show that -- contrary to the statement by
Sondhi et al. -- there is a closed form solution for the key quantity in
their approach termed ``Bayesian Additional Evidence tipping point''.
% The Bayesian Additional Evidence method provides an answer to the question
% ``What prior do I have to choose so that that my unconvincing data become
% convincing?''.
The method is also closely related to the Analysis of Credibility by
\citet{Matthews2018}. We investigate differences and similarities of the two
methods, showing that the priors determined through the Bayesian Additional
Evidence method are not always helpful.

R. Matthews alerted us about the article from \citet{Sondhi2021}. After reading
it, I realized that their statement about closed-form solutions was incorrect
and derived a solution. \mbox{L. Held} suggested to write a commentary. I wrote
an initial draft of the manuscript, which \mbox{R. Matthews} substantially
improved by expanding it with some logical flaws of the method. The two of us
iteratively worked on the manuscript, while L. Held gave high-level feedback.

\subsection{Integrity in methodological research}

\startPaperSummary{PaperVI}{%
  Pitfalls and Potentials in Simulation Studies}{%
  Samuel Pawel, Lucas Kook, Kelly Reeve \\
  2022. arXiv preprint.
  doi:\href{https://doi.org/10.48550/arXiv.2203.13076}{10.48550/arXiv.2203.13076}.}

Simulation studies are frequently used for evaluating statistical methods.
However, several studies showed that the reporting standards in simulation
studies have remained low over the years \citep{Hoaglin1975, Burton2006,
  Morris2019}. Moreover, some authors have recently argued that methodological
research is suffering from reproducibility issues, publication bias, and a
``replication crisis'' due to researchers engaging in questionable research
practices, such as selective reporting \citep{Boulesteix2020}. In this article,
draw attention to these issues. We summarize possible questionable research
practices in simulation studies, and show how easy it is to make a method seem
superior if various questionable research practices are employed. We also give
recommendations to alleviate these issues. Most importantly, we recommend
researchers to write and pre-register simulation protocols.

The first authorship of this manuscript is shared with L. Kook. I had the idea
to invent a novel method and use questionable research practices that make it
seem superior, to draw attention to the low standards in methodological
research. I then wrote a first draft of the manuscript and proposed the idea to
L. Kook and K. Reeve. L. Kook and myself then came up with the novel regression
method ``AINET'' and started writing the simulation protocol. L. Kook took lead
in developing the R package \texttt{ainet}
(\url{https://github.com/LucasKook/ainet}) and simulation study code. Myself, L.
Kook, and K. Reeve then designed the final simulation study and continued
working on the manuscript together. Recently, I was invited to present the
results from this project at the CEN (Central European Network) conference 2023
in Basel.


\phantomsection
\addcontentsline{toc}{section}{Data and software}
\section*{Data and software}
The CC-By 4.0 licensed data from the ``Reproducibility Project: Cancer Biology''
\citep{Errington2021} were downloaded from
\url{https://doi.org/10.17605/osf.io/e5nvr}. The relevant variables were then
extracted from the file ``\texttt{RP\_CB Final Analysis - Effect level
  data.csv}''. All analyses from this chapter were conducted in the R
programming language version \Sexpr{paste(version$major, version$minor, sep =
  ".")} \citep{R}. The packages \texttt{dplyr} \citep{Wickham2022},
\texttt{ggplot2} \citep{Wickham2016}, \texttt{knitr} \citep{Xie2022},
\texttt{ReplicationSuccess} \citep{Held2020}, \texttt{UpSetR}
\citep{Gehlenborg2019}, and \texttt{xtable} \citep{Dahl2019} were used. Code and
data to reproduce this thesis are available at
\url{https://doi.org/10.5281/zenodo.XXXXXXX}.


<< "sessionInfo1", eval = Reproducibility, results = "asis" >>=
## print R sessionInfo to see system information and package versions
## used to compile the manuscript (set Reproducibility = FALSE, to not do that)
cat("\\newpage \\section*{Computational details}")
@
<< "sessionInfo2", echo = Reproducibility, results = Reproducibility >>=
sessionInfo()
@

% Bibliography
\nocite{Pawel2020b, Held2021, Pawel2022c,Held2021b, Pawel2022d, Pawel2022e}
\bibliographystyle{apalikedoiurl}
\bibliography{bibliography}
